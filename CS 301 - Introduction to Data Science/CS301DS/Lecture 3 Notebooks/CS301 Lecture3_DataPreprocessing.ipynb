{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22ebf8d",
   "metadata": {},
   "source": [
    "# Data Preprocessing Steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c47a0",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1fdd3a",
   "metadata": {},
   "source": [
    "**numpy** will allow us to work with arrays, which will be expected as input for some machine models.\n",
    "\n",
    "**matplotlib** will allow us to plot charts and graphs.\n",
    "\n",
    "**pandas** will allow us to import the datasets, as well as create the matrix of features and the dependent variable \n",
    "vector. \n",
    "\n",
    "**sklearn.preprocessing** will allow us to scale features\n",
    "\n",
    "**sklearn.model_selection.train_test_split** will allow us to randomly split our data into a set for training and a set for testing\n",
    "\n",
    "**sklearn.impute** provides tools to allow us to deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db10320",
   "metadata": {},
   "source": [
    "## Import and examine the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa74ac",
   "metadata": {},
   "source": [
    "[pima-indians-diabetes.csv](https://drive.google.com/file/d/1lWDk46jRhhFg8xY6Ga8bH6-6sWOpfGpM/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18bcbbb",
   "metadata": {},
   "source": [
    "## Taking Care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2c183",
   "metadata": {},
   "source": [
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "\n",
    "1. Plasma glucose concentration\n",
    "2. Diastolic blood pressure\n",
    "3. Triceps skinfold thickness\n",
    "4. 2-Hour serum insulin\n",
    "5. Body mass index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa74ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = pd.Series(['plasma_glucose','bp','skin_fold','2_hr_insulin','bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of '0' values for each column\n",
    "num_missing = (df[cols_with_missing] == 0).sum()\n",
    "# report the results\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '0' values with 'nan'\n",
    "df[cols_with_missing] = df[cols_with_missing].replace(0, np.nan)\n",
    "\n",
    "# print the first 20 rows of data\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ca72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of nan values in each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a78aaf",
   "metadata": {},
   "source": [
    "### Alternative - eliminate any rows that contain NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc23b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the shape of the raw data\n",
    "print(df.shape)\n",
    "# drop rows with missing values\n",
    "df1 = df.dropna()\n",
    "# summarize the shape of the data with missing rows removed\n",
    "print(df1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05764fd3",
   "metadata": {},
   "source": [
    "### Alternative - impute missing values\n",
    "\n",
    "- A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "- A value from another randomly selected record.\n",
    "- A mean, median or mode value for the column.\n",
    "- A value estimated by another predictive model.\n",
    "- For time series data, average the previous and the succeeding values\n",
    "- For categorical data, create a new category.  For instance, 'Missing Gender' for a 'Sex' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with mean column values\n",
    "# is can also be done in place if desired\n",
    "df1 = df.fillna(df.mean())\n",
    "# count the number of NaN values in each column\n",
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1141c",
   "metadata": {},
   "source": [
    "#### The scikit-learn library provides the SimpleImputer pre-processing class that can be used to replace missing values. \n",
    "\n",
    "**Imputation strategies:**<br/>\n",
    "- **mean:** replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "- **median:** replace missing values using the median along each column. Can only be used with numeric data.\n",
    "- **most_frequent:** replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "- **constant:** replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "- **instance of Callable:** replace missing values using the scalar statistic returned by running the callable over a dense 1d array containing non-missing values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the numpy array\n",
    "values = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the imputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# transform the dataset\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "# count the number of NaN values in each column\n",
    "print('Missing: %d' % np.isnan(transformed_values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d2b3d",
   "metadata": {},
   "source": [
    "## Encoding Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd769e",
   "metadata": {},
   "source": [
    "[penguins2.csv](https://drive.google.com/file/d/16AOCJJar6igQC8v9KJeotX7wN3OuHMRx/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d580b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/penguins2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(inplace=True)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611fdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['bill_length_mm','bill_depth_mm','flipper_length_mm',\n",
    "                     'body_mass_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac945004",
   "metadata": {},
   "source": [
    "## Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e4005",
   "metadata": {},
   "source": [
    "#### Two categorical independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ae480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['island'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15947991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f9870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df2, columns=[\"island\",\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd243b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['island_Biscoe','island_Dream',\n",
    "                       'island_Torgersen','sex_female','sex_male']\n",
    "ind_columns = numeric_columns + categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73f319",
   "metadata": {},
   "source": [
    "## Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb092a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"label\"] = np.where(df2[\"species\"].str.contains(\"Adelie\"), 1, 0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1a6a4",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "**Feature Scaling** is necessary for distance-based machine learning algorithms such as kNN (k-Nearest Neighbors) and SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df34b5e",
   "metadata": {},
   "source": [
    "**Standardization** converts features to a range centered at 0, with 1 representing a standard deviation:\n",
    "\n",
    "$$ x_{standardized} = \\frac{x_{original} - \\mu_x}{\\sigma_x}  $$\n",
    "\n",
    "**$ \\mu_x $** is the mean and **$ \\sigma_x $** is the standard deviation of feature . The standardized value is called a z-score. Since each unit represents one standard deviation, most z-scores fall between -2 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29606e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df2[numeric_columns]\n",
    "# Standardize dataframe and return as an array\n",
    "standardizedArray = preprocessing.scale(original)\n",
    "\n",
    "# Convert standardized array to dataframe 'standardized'\n",
    "standardized = pd.DataFrame(standardizedArray, columns=numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7af12e",
   "metadata": {},
   "source": [
    "**Normalization** converts features to the range [0,1]:\n",
    "\n",
    "$$ x_{normalized} = \\frac{x_{original} - Min_x}{Max_x - Min_x} $$\n",
    "\n",
    "- Normalization is often used when a feature does not have a Guassian distribution.\n",
    "- Note that normalized outliers will be bound between [0,1], while standardization is unbounded and outliers will not be affected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataframe and return as an array\n",
    "normalizedArray = preprocessing.MinMaxScaler().fit_transform(df2[numeric_columns])\n",
    "\n",
    "# Convert normalized array to dataframe 'normalized'\n",
    "normalized = pd.DataFrame(normalizedArray, columns=numeric_columns)\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7e453",
   "metadata": {},
   "source": [
    "***Python data structuring methods.***\n",
    "| Method\t| Parameters\t| Description |\n",
    "| :-------- | :------------ | :---------- |\n",
    "| string[start:end]\t| none\t| Returns the substring of string that begins at the index start and ends at the index end - 1. |\n",
    "|string.capitalize()<br>string.upper()<br>string.lower()<br>string.title()\t| none\t| Returns a copy of string with the initial character uppercase, all characters uppercase, all characters lowercase, or the initial character of all words uppercase. |\n",
    "| to_datetime()\t| arg\t| Converts arg to datetime data type and returns the converted object. Data type of arg may be int, float, str, datetime, list, tuple, one-dimensional array, Series, or DataFrame. |\n",
    "| to_numeric()\t| arg\t| Converts arg to numeric data type and returns the converted object. Data type of arg may be scalar, list, tuple, one-dimensional array, or Series.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb926542",
   "metadata": {},
   "source": [
    "**pandas data structuring methods.**\n",
    "\n",
    "| Method\t| Parameters\t| Description |\n",
    "| :-------- | :------------ | :---------- |\n",
    "| df.astype()\t| dtype<br>copy=True\t| Converts the data type of all dataframe df columns to dtype. To alter individual columns, specify dtype as {col: dtype, col:dtype, . . .}. |\n",
    "| df.insert()\t| loc<br>column<br>value\t| Inserts a new column with label column at location loc in dataframe df. value is a Scalar, Series, or Array of values for the new column. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0d007",
   "metadata": {},
   "source": [
    "***Python data enriching methods.***\n",
    "| Method\t| Parameters\t| Description |\n",
    "| :-------- | :------------ | :---------- |\n",
    "| concat()\t|objs<br>axis=0<br>join='outer'<br>ignore_index=False\t| Appends dataframes specified in objs parameter. Appends rows if **axis=0** or columns if **axis=1**. join specifies whether to perform an 'outer' or 'inner' join. Resulting index values are unchanged if **ignore_index=False** or renumbered if **ignore_index=True**. |\n",
    "| df.apply()\t| func<br>axis=0<br>\t| Applies the function specified in func parameter to a dataframe df. Applies function to each column if **axis=0** or to each row if **axis=1**. Returns a Series or DataFrame. |\n",
    "| df.insert()\t| loc<br>column<br>value\t| Inserts a column to df. **loc** specifies the integer position of the new column. **column** specifies a string or numeric column label. **value** specifies column values as a Scalar or Series. |\n",
    "| df.merge()\t| right<br>how='inner'<br>on=None<br>sort=False\t|Joins df with the right dataframe. **how** specifies whether to perform a **'left'**, **'right'**, **'outer'**, or **'inner'** join.  **on** specifies join column labels, which must appear in both dataframes. If **on=None**, all matching labels become join columns. **sort=True** sorts rows on the join columns.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb710f",
   "metadata": {},
   "source": [
    "## Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store relevant columns as variables\n",
    "X = df2[ind_columns].values\n",
    "# create a 1-D numpy array\n",
    "y = df2[['label']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cadfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df2[ind_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "print('Split X: ',trainX.shape, testX.shape)\n",
    "print('Split Y: ',trainY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55174b",
   "metadata": {},
   "source": [
    "**Leading public datasets.**\n",
    "| Name\t| Link\t| Description |\n",
    "| :---- | :---- | :-----------|\n",
    "| Kaggle\t| kaggle.com\t| Over 50,000 datasets on a broad range of subjects. Also provides Jupyter notebooks that analyze the datasets. |\n",
    "| FiveThirtyEight\t| data.fivethirtyeight.com\t| Datasets on politics, sports, science, economics, health, and culture, initially developed to support FiveThirtyEight publications. |\n",
    "| University of California Irvine Machine Learning Repository |\tarchive.ics.uci.edu\t| 622 datasets, primarily in science, engineering, and business. |\n",
    "| Data.gov\t| data.gov\t| U.S. government datasets on agriculture, climate, energy, maritime, oceans, and health. |\n",
    "| World Bank Open Data\t| data.worldbank.org\t| Global datasets on subjects such as health, education, agriculture, and economics. |\n",
    "| Nasdaq Data Link\t| data.nasdaq.com\t| Financial and economic datasets. |\n",
    "| NYC Open Data\t| opendata.cityofnewyork.us\t| NYC government services datasets. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fdd75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
